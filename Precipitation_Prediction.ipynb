{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Precipitation-Prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxNdFBKWWN2TxFlUWfpMw3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnh03288/Precipitation-Prediction/blob/main/Precipitation_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwbHIcTweuLb"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Conv2DTranspose, MaxPooling2D, BatchNormalization, Activation, concatenate, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRibZgqFvscB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itZfmAIXkZ6g"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2KtLTfWLRgW"
      },
      "source": [
        "# 재생산성을 위해 시드 고정\n",
        "np.random.seed(7)\n",
        "random.seed(7)\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v47Uo1YeV-Yq"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/My Drive/강수량/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GHm-FppCVa_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s8nfyfRLRdB"
      },
      "source": [
        "train = []\n",
        "train_path = '/content/drive/My Drive/강수량'\n",
        "train_files = sorted(glob.glob(train_path + '/*'))\n",
        "i = 0\n",
        "print(i)\n",
        "for file in train_files:\n",
        "    dataset = np.load(file)\n",
        "    train.append(dataset)\n",
        "    i =i + 1\n",
        "    if i%10 ==0:\n",
        "      print(i)\n",
        "    \n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG16GgYTV4Lz"
      },
      "source": [
        "train = np.array(train)\n",
        "test = np.array(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn_rlZc5-K_F"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7OozllpK7fA"
      },
      "source": [
        "x_train = train[:,:,:,:10]\n",
        "y_train = train[:,:,:,14]\n",
        "\n",
        "#del train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dK0kMUDLRYf"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.025, random_state=7777)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb79Rtm6psNi"
      },
      "source": [
        "y_train_ = y_train.reshape(-1,y_train.shape[1]*y_train.shape[2])\n",
        "\n",
        "x_train = np.delete(x_train, np.where(y_train_<0)[0], axis=0)\n",
        "y_train = np.delete(y_train, np.where(y_train_<0)[0], axis=0)\n",
        "y_train = y_train.reshape(-1, x_train.shape[1], x_train.shape[2],1)\n",
        "y_test = y_test.reshape(-1, y_test.shape[1], y_test.shape[2],1)\n",
        "\n",
        "y_train_ = np.delete(y_train_, np.where(y_train_<0)[0], axis=0)\n",
        "\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Dyl2UKUl8M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj6LVHAHLRPt"
      },
      "source": [
        "x_train = x_train[np.sum(y_train_, axis=1)>= 50]\n",
        "y_train = y_train[np.sum(y_train_, axis=1)>= 50]\n",
        "\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAONLY8arc10"
      },
      "source": [
        "\n",
        "xy_savez_compress_load = np.load('/content/drive/My Drive/강수량/tmp.npz')\n",
        "type(xy_savez_compress_load)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3eLUtf9r0Vk"
      },
      "source": [
        "x_train = xy_savez_compress_load['x']\n",
        "y_train = xy_savez_compress_load['y']\n",
        "xy_savez_compress_load.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDVmq0Qhimfw"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCHUMvUcjZGU"
      },
      "source": [
        "def data_generator(x_train, y_train):\n",
        "    rotate_X_90 = np.zeros_like(x_train)\n",
        "    rotate_Y_90 = np.zeros_like(y_train)\n",
        "\n",
        "    for j in range(rotate_X_90.shape[0]):\n",
        "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
        "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
        "        for i in range(10):\n",
        "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
        "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
        "\n",
        "        rotate_X_90[j,:,:,:] = rotate_x\n",
        "        rotate_Y_90[j,:,:,:] = rotate_y\n",
        "\n",
        "    rotate_X_180 = np.zeros_like(x_train)\n",
        "    rotate_Y_180 = np.zeros_like(y_train)\n",
        "\n",
        "    for j in range(rotate_X_180.shape[0]):\n",
        "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
        "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
        "        for i in range(10):\n",
        "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
        "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
        "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
        "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
        "\n",
        "        rotate_X_180[j,:,:,:] = rotate_x\n",
        "        rotate_Y_180[j,:,:,:] = rotate_y\n",
        "\n",
        "    rotate_X_270 = np.zeros_like(x_train)\n",
        "    rotate_Y_270 = np.zeros_like(y_train)\n",
        "\n",
        "    for j in range(rotate_X_270.shape[0]):\n",
        "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
        "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
        "        for i in range(10):\n",
        "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
        "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
        "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
        "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
        "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
        "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
        "\n",
        "        rotate_X_270[j,:,:,:] = rotate_x\n",
        "        rotate_Y_270[j,:,:,:] = rotate_y\n",
        "\n",
        "    x_train = np.concatenate((x_train, rotate_X_90, rotate_X_180, rotate_X_270), axis = 0)\n",
        "    y_train = np.concatenate((y_train, rotate_Y_90, rotate_Y_180, rotate_Y_270), axis = 0)\n",
        "    del rotate_X_90, rotate_X_180, rotate_X_270\n",
        "\n",
        "    x_T = np.zeros_like(x_train)\n",
        "    y_T = np.zeros_like(y_train)\n",
        "\n",
        "    for i in range(x_train.shape[0]):\n",
        "        for j in range(x_train.shape[3]):\n",
        "            x_T[i,:,:,j] = x_train[i,:,:,j].T\n",
        "        y_T[i,:,:,0] = y_train[i,:,:,0].T\n",
        "\n",
        "    x_train = np.concatenate((x_train, x_T), axis = 0)\n",
        "    y_train = np.concatenate((y_train, y_T), axis = 0)\n",
        "\n",
        "    del x_T,y_T\n",
        "    \n",
        "    return x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy97eJ6bMq5q"
      },
      "source": [
        "def mae_over_fscore(y_true, y_pred):\n",
        "    '''\n",
        "    y_true: sample_submission.csv 형태의 실제 값\n",
        "    y_pred: sample_submission.csv 형태의 예측 값\n",
        "    '''\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_true = y_true.reshape(1, -1)[0]  \n",
        "    \n",
        "    y_pred = np.array(y_pred)\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    \n",
        "    # 실제값이 0.1 이상인 픽셀의 위치 확인\n",
        "    IsGreaterThanEqualTo_PointOne = y_true >= 0.1\n",
        "    \n",
        "    # 실제 값에 결측값이 없는 픽셀의 위치 확인 \n",
        "    IsNotMissing = y_true >= 0\n",
        "    \n",
        "    # mae 계산\n",
        "    mae = np.mean(np.abs(y_true[IsGreaterThanEqualTo_PointOne] - y_pred[IsGreaterThanEqualTo_PointOne]))\n",
        "    \n",
        "    # f1_score 계산 위해, 실제값에 결측값이 없는 픽셀에 대해 1과 0으로 값 변환\n",
        "    y_true = np.where(y_true[IsNotMissing] >= 0.1, 1, 0)\n",
        "    \n",
        "    y_pred = np.where(y_pred[IsNotMissing] >= 0.1, 1, 0)\n",
        "    \n",
        "    # f1_score 계산    \n",
        "    f_score = f1_score(y_true, y_pred) \n",
        "    # f1_score가 0일 나올 경우를 대비하여 소량의 값 (1e-07) 추가 \n",
        "    return mae / (f_score + 1e-07) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMGtIHi8Mt4x"
      },
      "source": [
        "def mae(y_true, y_pred):    \n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    \n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    \n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    \n",
        "    over_threshold = y_true >= 0.1\n",
        "    \n",
        "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
        "\n",
        "def fscore(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    \n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    \n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    \n",
        "    remove_NAs = y_true >= 0\n",
        "    \n",
        "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
        "    \n",
        "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
        "    \n",
        "    return(f1_score(y_true, y_pred))\n",
        "\n",
        "def maeOverFscore(y_true, y_pred):\n",
        "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
        "\n",
        "def fscore_keras(y_true, y_pred):\n",
        "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
        "    return score\n",
        "\n",
        "def score(y_true, y_pred):\n",
        "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJkFIs2oLpEo"
      },
      "source": [
        "def create_model():\n",
        "    inputs=Input(x_train.shape[1:])\n",
        "    \n",
        "    bn=BatchNormalization()(inputs)\n",
        "    conv0=Conv2D(256, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
        "    \n",
        "    bn=BatchNormalization()(conv0)\n",
        "    conv=Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
        "    concat=concatenate([conv0, conv], axis=3)\n",
        "    \n",
        "    bn=BatchNormalization()(concat)\n",
        "    conv=Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
        "    concat=concatenate([concat, conv], axis=3)\n",
        "        \n",
        "    for i in range(5):\n",
        "        bn=BatchNormalization()(concat)\n",
        "        conv=Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
        "        concat=concatenate([concat, conv], axis=3)\n",
        "    \n",
        "    bn=BatchNormalization()(concat)\n",
        "    outputs=Conv2D(1, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
        "    \n",
        "    model=Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c28MQgYz8GHa"
      },
      "source": [
        "#x_train, y_train = data_generator(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTUz0ef68F_W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4He3-K33vUNy"
      },
      "source": [
        "model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAKbG3ijZNP"
      },
      "source": [
        "model.compile(loss='mae', optimizer='adam', metrics=[score, fscore_keras])\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6w0xLjmLwTh"
      },
      "source": [
        "model.save('/content/drive/My Drive/강수량/model1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2N71KHyN-yc"
      },
      "source": [
        "test = []\n",
        "for sub_id in submission['id']:\n",
        "    data = np.load('/content/drive/My Drive/강수량/test/'+'subset_'+sub_id+'.npy').astype('float32')\n",
        "    test.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2XKqOrROR2i"
      },
      "source": [
        "test = np.array(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0R4KGg3OvjY"
      },
      "source": [
        "test = test[:,:,:,:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI8LYgBZTz4E"
      },
      "source": [
        "pred = model.predict(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAOHv5dPNJIB"
      },
      "source": [
        "submission.iloc[:,1:] = pred.reshape(-1,1600)\n",
        "submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPQASpAlLv0H"
      },
      "source": [
        "submission.to_csv('/content/drive/My Drive/강수량/submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}